{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custum_frozen_lake_env import CustumFrozenLakeEnv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_q_value(value, title=\"State Action Value (Q) Function\", fig_size = 12, char_size = 8): # Visualize q value\n",
    "    n_state, n_action = value.shape\n",
    "\n",
    "    # Triangle patches for each action\n",
    "    lft_tri = np.array([[0,0],[-0.5,-0.5],[-0.5,0.5]])\n",
    "    dw_tri = np.array([[0,0],[-0.5,0.5],[0.5,0.5]])\n",
    "    up_tri = np.array([[0,0],[0.5,-0.5],[-0.5,-0.5]])\n",
    "    rgh_tri = np.array([[0,0],[0.5,0.5],[0.5,-0.5]])\n",
    "\n",
    "    # Color\n",
    "    high_color = np.array([1,1,0,0.8])\n",
    "    low_color = np.array([0,0,1,0.8])\n",
    "    \n",
    "    fig = plt.figure(num=0,figsize=(fig_size,fig_size))\n",
    "    plt.title(title,fontsize=char_size*2)  \n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            s = i*8+j\n",
    "            min_q = np.min(value[s])#-1.00321235238\n",
    "            max_q = np.max_q(value[s])#0.5868902419\n",
    "            for a in range(n_action):\n",
    "                val = value[s,a]\n",
    "                ratio = (val - min_q)/(max_q - min_q + 1e-10) # reward color (red : high, blue : low)\n",
    "                if ratio > 1: clr = high_color\n",
    "                elif ratio < 0: clr = low_color\n",
    "                else: clr = high_color*ratio + low_color*(1-ratio)\n",
    "\n",
    "                if a == 0: # Left arrow\n",
    "                    plt.gca().add_patch(plt.Polygon([j,i]+lft_tri, color=clr, ec='k'))\n",
    "                    plt.text(j-0.4, i+0.1, \"{:2.3f}\".format(val), fontsize=char_size)\n",
    "                if a == 1: # Down arrow\n",
    "                    plt.gca().add_patch(plt.Polygon([j,i]+dw_tri, color=clr, ec='k'))\n",
    "                    plt.text(j-0.1, i+0.4, \"{:2.3f}\".format(val), fontsize=char_size)\n",
    "                if a == 2: # Right arrow\n",
    "                    plt.gca().add_patch(plt.Polygon([j,i]+rgh_tri, color=clr, ec='k'))\n",
    "                    plt.text(j+0.2, i+0.1, \"{:2.3f}\".format(val), fontsize=char_size)\n",
    "                if a == 3: # Up arrow\n",
    "                    plt.gca().add_patch(plt.Polygon([j,i]+up_tri, color=clr, ec='k'))\n",
    "                    plt.text(j-0.1, i-0.2, \"{:2.3f}\".format(val), fontsize=char_size)\n",
    "    plt.xlim([-0.5,7.5])\n",
    "    plt.xticks(range(8))\n",
    "    plt.ylim([-0.5,7.5])\n",
    "    plt.yticks(range(8))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "def display_episode(fig_size = 8, char_size = 8):\n",
    "    # Triangle patches for each action\n",
    "    lft_tri = np.array([[0,0],[-0.5,-0.5],[-0.5,0.5]])\n",
    "    dw_tri = np.array([[0,0],[-0.5,0.5],[0.5,0.5]])\n",
    "    up_tri = np.array([[0,0],[0.5,-0.5],[-0.5,-0.5]])\n",
    "    rgh_tri = np.array([[0,0],[0.5,0.5],[0.5,-0.5]])\n",
    "\n",
    "    # Color\n",
    "    arr_len = 0.2\n",
    "\n",
    "    fig = plt.figure(num=0,figsize=(fig_size,fig_size))\n",
    "    plt.title(\"Episode\",fontsize=char_size*2)  \n",
    "    for state, action, reward, next_state, next_action, done in episode:\n",
    "        j = state%8\n",
    "        i = (state - j)/8\n",
    "\n",
    "        if action == 0: # Left arrow\n",
    "            plt.arrow(j,i,-arr_len,0,color=\"r\",alpha=1,width=0.01,head_width=0.5,head_length=0.1,overhang=1)\n",
    "        if action == 1: # Down arrow\n",
    "            plt.arrow(j,i,0,arr_len,color=\"r\",alpha=1,width=0.01,head_width=0.5,head_length=0.1,overhang=1)\n",
    "        if action == 2: # Right arrow\n",
    "            plt.arrow(j,i,arr_len,0,color=\"r\",alpha=1,width=0.01,head_width=0.5,head_length=0.1,overhang=1)\n",
    "        if action == 3: # Up arrow\n",
    "            plt.arrow(j,i,0,-arr_len,color=\"r\",alpha=1,width=0.01,head_width=0.5,head_length=0.1,overhang=1)\n",
    "\n",
    "    plt.xlim([-0.5,7.5])\n",
    "    plt.xticks(range(8))\n",
    "    plt.ylim([-0.5,7.5])\n",
    "    plt.yticks(range(8))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustumFrozenLakeEnv(map_name=\"8x8\")\n",
    "\n",
    "obs_space = env.observation_space\n",
    "n_state = obs_space.n\n",
    "print('Observation space')\n",
    "print(\"Total {} states\".format(n_state))\n",
    "\n",
    "act_space = env.action_space\n",
    "n_action = act_space.n\n",
    "print('Action space')\n",
    "print(\"Total {} actions\".format(n_action))\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration with State Action Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration find the optimal value function of MDPs using \n",
    "\n",
    "$V_{k+1}(s) = \\max_{a} \\left[r(s,a,s') + \\gamma V_{k}(s') \\right] P(s'|s,a)$\n",
    "\n",
    "However, we need model to obtain the optimal policy from the optimal value function\n",
    "\n",
    "$\\pi'(a|s) = \\arg\\max_{a} \\sum_{s'} \\left(r(s,a,s') + V(s')\\right) P(s'|s,a)$\n",
    "\n",
    "If we compute Q(s,a) instead of V(s), then, we don't need a model for policy improvement\n",
    "\n",
    "Greedy policy improvement over Q(s,a) is model free\n",
    "\n",
    "$\\pi'(a|s) = \\arg\\max_{a} Q(s,a)$\n",
    "\n",
    "Before looking model free value iteration, let's see how value iteration works with state action value (Q value) when a model is given\n",
    "\n",
    "To implement value iteration with Q value, we have to use the Bellman eqaution for Q value as follows:\n",
    "\n",
    "$Q_{k+1}(s,a) = \\sum_{s'} \\left[r(s,a,s') + \\gamma \\max_{a'} Q_{k}(s',a') \\right] P(s'|s,a)$\n",
    "\n",
    "Algorithm\n",
    "--\n",
    "---\n",
    "- $V_{k}(s) = \\max_{a'} Q_{k}(s,a')$\n",
    "\n",
    "- $Q_{k+1}(s,a) = \\sum_{a'} \\left[r(s,a,s') + \\gamma V_{k}(s') \\right] P(s'|s,a)$\n",
    "\n",
    "The optimal policy\n",
    "\n",
    "- $\\pi(a|s) = \\mathbf{1}\\left(a = \\max_{a'} Q(s,a')\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_value_iteration(env,gamma=0.999,epsilon=1e-6):\n",
    "    \n",
    "    # Extract environment information\n",
    "    obs_space = env.observation_space\n",
    "    n_state = obs_space.n\n",
    "    act_space = env.action_space\n",
    "    n_action = act_space.n\n",
    "    \n",
    "    P = np.zeros((n_state,n_action,n_state))\n",
    "    r = np.zeros((n_state,n_action,n_state))\n",
    "    for s in env.unwrapped.P.keys(): # For all states s, update v(s)\n",
    "        for a in env.unwrapped.P[s].keys(): # For all actions a\n",
    "            for prob, next_s, reward, done in env.unwrapped.P[s][a]: # For all possible transitions (s,a,s')\n",
    "                P[s][a][next_s]=prob\n",
    "                r[s][a][next_s]=reward\n",
    "\n",
    "    q = np.random.uniform(size=(n_state,n_action))\n",
    "    \n",
    "    while True: \n",
    "        v = np.max(q,axis=1)\n",
    "        q_prime = np.sum((r+gamma*np.tile(v[np.newaxis,np.newaxis,:],reps=(n_state,n_action,1)))*P,axis=2)\n",
    "        dist = np.max(np.max(np.abs(q-q_prime)))\n",
    "        q = q_prime\n",
    "        if dist < epsilon:\n",
    "            break\n",
    "\n",
    "    policy = np.zeros((n_state,n_action))\n",
    "    policy[np.arange(n_state),np.argmax(q,axis=1)] = 1\n",
    "    \n",
    "    return policy, q\n",
    "\n",
    "if __name__=='__main__': \n",
    "    policy,q = q_value_iteration(env)\n",
    "    display_q_value(q,title=\"Policy Iteration with State Action Value (Q)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model-free policy iteration,\n",
    "we need to estimate the following update using samples!!\n",
    "\n",
    "$Q_{k+1}(s,a) = \\sum_{s'} \\left[r(s,a,s') + \\gamma \\max_{a'} Q_{k}(s',a') \\right] P(s'|s,a)$\n",
    "\n",
    "Update Q value using temporal difference (TD) target and error!!\n",
    "\n",
    "$Q_{new}(S_{t}, A_{t}) \\leftarrow Q_{old}(S_{t}, A_{t}) + \\alpha (R_{t+1} + \\gamma \\max_{a'} Q_{old}(S_{t+1},a') - Q_{old}(S_{t}, A_{t}))$\n",
    "\n",
    "We can update our estimator online, just $(S_{t},A_{t},R_{t+1},S_{t+1})$ is needed.\n",
    "$A_{t+1}$ is not needed.\n",
    "\n",
    "TD target is $R_{t+1} + \\gamma \\max_{a'} Q_{old}(S_{t+1},a')$\n",
    "\n",
    "TD error is $R_{t+1} + \\gamma \\max_{a'} Q_{old}(S_{t+1},a') - Q(S_{t},A_{t})$\n",
    "\n",
    "Algorithm\n",
    "--\n",
    "---\n",
    "For every time step\n",
    "\n",
    "Policy Evaluation\n",
    "\n",
    "- $(S_{t},A_{t},R_{t+1},S_{t+1}$\n",
    "\n",
    "- $TDtarget = R_{t+1} + \\gamma \\max_{a'} Q_{old}(S_{t+1},a')$\n",
    "\n",
    "- $TDerror = R_{t+1} + \\gamma \\max_{a'} Q_{old}(S_{t+1},a') - Q(S_{t},A_{t})$\n",
    "\n",
    "- $Q[S_{t}, A_{t}] = Q[S_{t}, A_{t}] + \\alpha TDerror$\n",
    "\n",
    "Policy Improvement\n",
    "\n",
    "- $\\pi(a|s) = \\frac{\\epsilon}{m} + (1-\\epsilon) \\mathbf{1}\\left(a = \\max_{a'} Q(s,a')\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent():\n",
    "    def __init__(self, n_state, n_action, alpha=0.9, gamma=0.999,\n",
    "                 exploration=\"EpsGrdy\", epsilon=1.0):\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        self.q = np.zeros([n_state,n_action])\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.exploration = exploration\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "    def update_value(self, state, action, reward, next_state, done):\n",
    "        q_old = self.q[state][action]\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.q[next_state])\n",
    "        td_error = td_target - q_old\n",
    "        self.q[state, action] = q_old + self.alpha * td_error\n",
    "\n",
    "    def update_policy(self, update_rate):\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            self.epsilon = self.epsilon*np.min([update_rate,1])\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            if np.random.uniform() < self.epsilon:\n",
    "                action = np.random.randint(0, high=self.n_action)\n",
    "            else:\n",
    "                action = np.argmax(self.q[state])\n",
    "        elif self.exploration == \"Random\":\n",
    "            action = np.random.randint(0, high=self.n_action)\n",
    "        return action\n",
    "    \n",
    "if __name__=='__main__': \n",
    "    qlearningagent = QLearningAgent(n_state,n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 3000\n",
    "for episode in range(n_episode):\n",
    "    state = env.reset()\n",
    "    action = qlearningagent.get_action(state)\n",
    "    done = False\n",
    "    while not done:    \n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_action = qlearningagent.get_action(next_state)\n",
    "\n",
    "        qlearningagent.update_value(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    qlearningagent.update_policy(1000/(episode+1))\n",
    "display_q_value(qlearningagent.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QlambdaLearningAgent():\n",
    "    def __init__(self, n_state, n_action, alpha=0.05, gamma=0.999, lam=0.6, exploration=\"EpsGrdy\", epsilon=1.0):\n",
    "        \n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        \n",
    "        self.q = np.zeros([n_state,n_action])\n",
    "        self.eligibility_trace = np.zeros([n_state,n_action])\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.exploration = exploration\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            self.epsilon = epsilon\n",
    "\n",
    "    def update_value(self, state, action, reward, next_state, done):\n",
    "        q_old = self.q[state][action]\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * np.max(self.q[next_state])\n",
    "        td_error = td_target - q_old\n",
    "        \n",
    "        self.eligibility_trace[state][action] += 1\n",
    "        \n",
    "        self.q = self.q + self.alpha * td_error * self.eligibility_trace\n",
    "        \n",
    "        self.eligibility_trace = self.gamma * self.lam * self.eligibility_trace\n",
    "        \n",
    "    def update_policy(self, update_rate):\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            self.epsilon = self.epsilon*np.min([update_rate,1])\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.exploration == \"EpsGrdy\":\n",
    "            if np.random.uniform() < self.epsilon:\n",
    "                action = np.random.randint(0, high=self.n_action)\n",
    "            else:\n",
    "                action = np.argmax(self.q[state])\n",
    "        elif self.exploration == \"Random\":\n",
    "            action = np.random.randint(0, high=self.n_action)\n",
    "        return action\n",
    "       \n",
    "if __name__=='__main__': \n",
    "    qlambdaagent = QlambdaLearningAgent(n_state,n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 1000\n",
    "for episode in range(n_episode):\n",
    "    state = env.reset()\n",
    "    action = qlambdaagent.get_action(state)\n",
    "    done = False\n",
    "    while not done:    \n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_action = qlambdaagent.get_action(next_state)\n",
    "\n",
    "        qlambdaagent.update_value(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    qlambdaagent.update_policy(1000/(episode+1))\n",
    "display_q_value(qlambdaagent.q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
